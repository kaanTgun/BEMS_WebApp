<!DOCTYPE html>
<html lang="en">
<head>
	<meta name="google-site-verification" content="h8gXNQb3ER85Zp6kUPWLvHCywhwYNpMwObgoJKDxWLE" />
	<meta charset="UTF-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="author" content="Kaan Turker Gun">
	<meta name="university" content="Carleton University Sustainable and Renewable Energy Engineering SREE Canada Ontario">
	<meta name="description" content="Battery Energy Management System, Smart Grid, micro grid">


	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta2/dist/css/bootstrap.min.css" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
	<link href="src/loader.css" rel="stylesheet">
	<link href="src/style.css" rel="stylesheet">
	<link href="src/styleSlideBar.css" rel="stylesheet">

	
	<title>Smart Battery Storage</title>
</head>
<body>

		<div class="page-header">
			<img class="logo" src="Assets/logo.png" style="width: 120px;" />
			<h1>Smart Battery Storage</h1>
		</div>

		<div class="page-holder bg-cover"
		style="background: url('Assets/Background.png') 50% 0 no-repeat fixed; background-size:cover; position: relative; z-index: 1;">
		
			<div class="container row align-items-center">
												<!-- Header START -->
				<div class="col-lg-5 col-md-6" style="margin-left: 30px; margin-top: 60px;">
					<h3 style="margin-top: 0px;"> 
						<b>
						About this project 
						</b> 
					</h3>
					<p style="margin-bottom:400px">
						The electricity grid functions in a real-time environment where the demand must be met by supply 
						at all time. With the increased adaptation of renewable resources, it is more important than ever 
						to balance the power grid and avoid any power shortages. 
						This project is an interactive and visual representation of control strategies used to solve the energy arbitrage problem. 
					</p>
				</div>
			</div>
		</div>

		<div style="background-color:#EFD57F;">
			<div class="container">
				<h4 style="padding-top: 40px;">
					What is the energy arbitrage problem?
				</h4>
				<div>
					<p>
					The idea is simple, power grid functions like the real-time market. 
					The price of power depends on the balance of supply and demand.
					If the demand is low, the price of power drops, shifting the power production to lower-cost resources, such as wind, solar, hydro or nuclear. 
					When the demand is high, the grid operator either spins up less efficient generators or buy power from other 
					operators to compensate for the demand causing the price of power to increase. 
					<br/>
					<br/>
					The increased penetration of renewables is causing unpredictability in power generation which conveys an unstable 
					grid and swinging electricity prices. With the help of large energy storage solutions, these problems can be overcome 
					allowing for a larger renewable presence. One of the more favored energy storage solutions is grid integrated batteries 
					due to their fast response time. Assuming the existence of the grid integrated batteries, with the historic hourly electricity data,
					an algorithm is needed to decide to buy, sell or hold power in a given time. Finding an optimal solution becomes especially challenging 
					due to the unpredictability of the price of power. The purpose of this project is to compare various algorithms 
					and strategies to find the optimal control strategy.
					</p>
				</div>
				<h4 style="padding-top: 40px;">
					<b>
						Few things to keep in mind:
					</b>
				</h4>
				<ul style="padding-bottom: 40px;">
					<li>
						Battery operation capacity is limited to minimum of 20% to maximum of 80%
					</li>
					<li>
						Assume the battery can be charged and discharged only 10% per hour
					</li>
					<li>
						The dataset used is available in Independent Electricity System Operator (IESO)
					</li>
				</ul>
			</div>
		</div>
											<!-- END -->
											<!-- Discriptions-1 START -->
		<div style="background-color: #A1CFCF; padding-top: 50px;">
			<div class="container">
				<div>
					<h5> 
						<b>Linear Programming (LP)</b>
					</h5>
					<p>
						LP is a methodology used to find a global optimum in a linearly defined space. 
						Which in this case, assuming the price of power is known for a defined period, 
						it is possible to find the optimal charging strategy. Thus, the maximum amount 
						of reward or money that can be made is determined by this optimal solution. In a 
						real-world scenario, it is impossible to predict the feature perfectly. 
						However, it can be used as a benchmark to compare the performances of other control strategies.
					</p>
					<img src="Assets/LP_equations.png" class="img-fluid" alt="Responsive image">
				</div>
				<div class="row align-items-center" style="padding-bottom: 50px;" >
					<div class="clearfix"></div>
					<div class="col-lg-8 col-md-7 col-sm-6 col-xs-12" >
						<h5 style="margin-top: 22px;"> 			
							<b>Exponential Mean Average (EMA)</b> 
						</h5>
						<div class="align-self-center">
							<p class="text-justify">
								EMA is a type of mean average calculation with the difference of, placing a larger 
								weight on the most recent data points. Compared to linear programming, this strategy 
								does not require to hold knowledge on the feature. It requires a limited number of past 
								observations to determine if the current price is higher or lower than the average. When 
								the price dips below the EMA and the battery is below the max SOC, 
								it starts charging, and the other way around. Although it sounds simple, this strategy is widely used by day-traders.
							</p>
						</div>

					</div>
					<div class="col-lg-4 col-md-5 col-sm-6 col-xs-12">
						<img src="Assets/EMA.png" class="img-fluid" alt="Responsive image" style="border-radius: 7px;"/>
					</div>
					<img src="Assets/EMA_equations.png" class="img-fluid" alt="Responsive image" style="width: 900px;">

				
				<div class="row align-items-center" style="padding-bottom: 50px;">

					<div class="col-lg-8 col-md-7 col-sm-6 col-xs-12" >
						<div>
							<h5>
								<b>
									Reinforcement Learning (RL)
								</b>
							</h5>
							<p>
								Reinforcement Learning is not a new concept; however, it became famous after 
								DeepMind team published a groundbreaking paper called ‚ÄòHuman-Level Control Through 
								Deep Reinforcement Learning‚Äô in 2015. They found a solution to successfully coupled neural 
								networks with reinforcement learning and achieve human-level control in Atari games. 
							</p>
						</div>
						<div>
							<h5>
								<b>
									Deep Q-Learning
								</b>
							</h5>
							<p>
								DQL consists of an agent (actor) and an environment. The agent intakes observations 
								from the environment evaluate its‚Äô current polity and return the best possible action. 
								This action produces a new state and reward, by the environment. 
								This cycle continues until the end of the episode. The goal of the agent is to maximize the reward received at each episode. 
							</p>
							<p>
								In this project, the state variables are set as the current price-of-power, 
								time, month and the state-of-charge. The agents‚Äô action space is limited to buying,
								selling or stalling electricity. Considering the size of the environment and the need to evaluate unseen states, 
								a differentiable policy is needed. Thus, a fully connected two-layer neural network was used as the policy network.
								The word ‚Äúdeep‚Äù in DQN comes from the existence of deep learning.  
							</p>	
							<p>
								Deep Q-Learning is a model-free reinforcement learning technique meaning, 
								the agent does not require a predefined knowledge of the environment. To optimize its policy, 
								DQL uses experience replay and Temporal Difference (TD) error, taking advantage of the episodic environment.
							</p>	
						</div>
						<div class="row justify-content-center">
							<div class="col-lg-10 col-md-12 col-sm-12">
								<img src="Assets/DQN_equation.png">
							</div>
						</div>
					</div>
					
					<div class="col-lg-4 col-md-5 col-sm-6 col-xs-12 " >
						<div class="row align-items-center">
							<img src="Assets/RL.png " class="img-fluid" alt="Responsive image" style="margin-top: 30px;">
							<img src="Assets/NN.png" class="img-fluid" alt="Responsive image" style="margin-top: 30px;">
						</div>
					</div>
				</div>

				</div>

			</div>		
		</div>
											<!-- END -->
											<!-- Graph START -->
		<div class="container">
			<div class="toggleButtons" style="margin-top: 40px;"> 
				<div class="form-check form-check-inline">
					<input class="form-check-input" type="checkbox" id="LP_Checkbox" onclick="LP_CheckboxEvent()" >
					<label class="form-check-label" for="inlineCheckbox3">Linear Programming üìà </label>
				</div>
				<div class="form-check form-check-inline">
					<input class="form-check-input" type="checkbox" id="EMA_Checkbox"  onclick="EMA_CheckboxEvent()">
					<label class="form-check-label" for="inlineCheckbox2">Exponential Moving Average üìä </label>
				</div>
				<div class="form-check form-check-inline">
					<input class="form-check-input" type="checkbox" id="DQN_Checkbox" onclick="DQN_CheckboxEvent()">
					<label class="form-check-label" for="inlineCheckbox1">Deep Q-Network ü§ñ </label>
				</div>
				<div class="form-check form-check-inline">
					<input class="form-check-input" type="checkbox" id="DDQN_Checkbox" onclick="DDQN_CheckboxEvent()">
					<label class="form-check-label" for="inlineCheckbox1">Double Deep Q-Network ü§ñ ü§ñ</label>
				</div>
			</div>
		</div>
		
		<div class="container" id='chart_1_container' style="margin-bottom: 40px;"> 
			<div class="row justify-content-center align-items-center" style="margin-bottom: 0px;" >
				<div class="col-lg-8 col-md-12 col-sm-12" >
					<canvas id="chart_1" style="display:none;"></canvas>
				</div>
				<div class="col-lg-4 col-md-12 col-sm-12">
					<canvas id="chart_2" ></canvas>
				</div>
			</div>

			<div class="loader" id="loader_1"></div>

			<div class="slidecontainer">
				<input type="range" min="24" value="50" class="slider" oninput="DateDraged()" onchange="DateChanged()" >
				<h3 class="currentDate" style="text-align: center;"></h3>

				<div class="row justify-content-center">
					<div class="btn-group btn-group-toggle " data-toggle="buttons" style="width: 400px; ">
					<label class="btn btn-secondary" >
						<input type="radio" name="HourOptions" id="graphXAxis_24h" autocomplete="off" value='24' checked onclick="DateChanged()"> 24hr
					</label>
					<label class="btn btn-secondary" >
						<input type="radio" name="HourOptions" id="graphXAxis_48h" autocomplete="off" value='48' onclick="DateChanged()"> 48hr
					</label>
					<label class="btn btn-secondary" >
						<input type="radio" name="HourOptions" id="graphXAxis_72h" autocomplete="off" value='72' onclick="DateChanged()"> 72hr
					</label>
					</div>
				</div>
				
			</div>
		</div>
										<!-- END -->
										<!-- Discriptions-2 START -->
		<div style="background-color: #A1CFCF; padding-top: 50px;">
			<div class="container" style="margin-top: 20px;"> 
				<div class="row justify-content-center ">

					<div class="col-lg-6 col-md-12 col-sm-12">
						<h4><b>Environment:</b></h4> 
						<p>
							Using a grid search method, 36 agents were trained to 
							determine the impact of the hyperparameters on the optimal policy. 
							Three major environment strategies were set and the influence on the final 
							optimal strategy can be seen. Also, for each DQN and DDQN strategy three episode lengths, 
							(24-48)hr, (48-100)hr and (100-200)hr were testes. 
							While the DDQN thrived in the long episodic environments, DQN suffered significantly from maximization bias.
						</p>
						<div style="justify-content: center;">
							<h5><b>Strategy 1:</b></h5>
							<p>
								The reward function is simple, the amount 
								of power bought (-) or sold (+) multiplied by the current price.
							</p>
							<div class="row justify-content-center " style="margin-bottom: 5px;">
								<div class="col-lg-8 col-md-12 col-sm-12">
									<img src="Assets/Strat1_pseudocode.png">
								</div>
							</div>
						</div>
					</div>

					<div class="col-lg-6 col-md-12 col-sm-12" id='changeDQNStrategy'>
						<div class="row justify-content-center ">
							<h3> Change Environment Variables </h3> 
							<br/>	
							<br/>	
							<br/>		
							<div class="Strategies col-lg-4 col-md-4 col-sm-4 col-xs-4">
								<H4>DQN & DDQN <br/> Strategy </H4>
								<div class="form-check">
									<input class="form-check-input" type="radio" name="StrategyInput" value="S1" checked>
									<label class="form-check-label" for="exampleRadios1">
										1
									</label>
								</div>
								<div class="form-check">
									<input class="form-check-input" type="radio" name="StrategyInput" value="S2" >
									<label class="form-check-label" for="exampleRadios2">
										2
									</label>
								</div>
								<div class="form-check disabled">
									<input class="form-check-input" type="radio" name="StrategyInput" value="S3" >
									<label class="form-check-label" for="exampleRadios3">
										3
									</label>
								</div>
							</div>
							<div class="EMAPeriods col-lg-4 col-md-4 col-sm-4 col-xs-4">
								<H4>EMA Periods</H4>
								<div class="form-check">
									<input class="form-check-input" type="radio" name="emaInput" value="24hr" checked>
									<label class="form-check-label" for="exampleRadios1">
										24-hr
									</label>
								</div>
								<div class="form-check">
									<input class="form-check-input" type="radio" name="emaInput" value="12hr" >
									<label class="form-check-label" for="exampleRadios2">
										12-hr
									</label>
								</div>
								<div class="form-check disabled">
									<input class="form-check-input" type="radio" name="emaInput" value="6hr" >
									<label class="form-check-label" for="exampleRadios3">
										6-hr
									</label>
								</div>
							</div>
							<div class="Decays col-lg-4 col-md-4 col-sm-4 col-xs-4">
								<H4>Battery Decay</H4>
								<div class="form-check">
									<input class="form-check-input" type="radio" name="decayInput" value="e003" checked>
									<label class="form-check-label" for="exampleRadios1">
										exp(-0.03x)
									</label>
								</div>
								<div class="form-check">
									<input class="form-check-input" type="radio" name="decayInput" value="e001" >
									<label class="form-check-label" for="exampleRadios2">
										exp(-0.01x)
									</label>
								</div>
		
							</div>

						</div>
						<div class="row justify-content-center">
							<button class="btn btn-primary" style="width: 200px; background-color: #8D88F2; border-width: 0px;" onclick="changeDQNStrategyEvent()">
								Change<br/>Parameters
							</button>
						</div>						
					</div>
				</div>

				<div class="row justify-content-center ">
					<div class="col-lg-6 col-md-12 col-sm-12">
						<h5><b>Strategy 2:</b></h5>
						<p>
							The reward function includes a 
							coefficient determined by EMA. This coefficient rewards the 
							agent if the power bought below average or sold over the average price. 
						</p>
						<div class="row justify-content-center ">
							<div class="col-lg-8 col-md-12 col-sm-12">
								<img src="Assets/Strat2_pseudocode.png" style="margin-bottom: 5px;">
							</div>
						</div>
					</div>
					<div class="col-lg-6 col-md-12 col-sm-12">
						<h5><b>Strategy 3:</b></h5>
						<p>
							The reward function includes a coefficient determined by the combination 
							of EMA and the cycle decay. The cycle decay was modelled by a negative exponential 
							function. With each taken action, the battery cycle was counted, and the cycle decay
							was determined and added to the reward coefficient.  
						</p>
						<div class="row justify-content-center">
							<div class="col-lg-8 col-md-12 col-sm-12">
								<img src="Assets/Strat3_pseudocode.png" style="margin-bottom: 15px;">
							</div>
						</div>
					</div>
				</div>
			</div>
		</div>

		<br/>

		<div style="background-color: #EFD57F">
			<div class="container">
				<br/>
				<br/>
				<h2 style="text-align: center;"> <b> Results and Findings</b></h2>
				<br/>
				<ul>
					<li>Test dataset ranging from 2015-05-15 02:00:00 to 
						2015-08-04 20:00:00 consists of 2000 hr of price data.</li>
						<li>For each control system, the initial SOC was set at 60%. 
						</li>
				</ul>

				<div class="row justify-content-center ">
					<div  class="col-lg-6 col-md-6 col-sm-12">
						<br/>
						<h4> <b> Linear Programming </b></h4>
						<p>
							The test dataset is large to find the optimal solution, however, 
							it was estimated to be 14219.91 $ with 0.00001 tolerance. 
							To obtain this value, the battery cycle count was calculated to be 81. 
							<br/>
							<br/>

						</p>
						<img src="Assets/LP_test_result.png" style="max-width: 100%; max-height: 100%;">
					</div>


					<div  class="col-lg-6 col-md-6 col-sm-6">
						<br/>
						<h4> <b> Exponential Mean Average </b></h4>
						<p>
							Four window sizes were tested, ranging from 6hr to 48hr. 
							By using a window size of 6hrs, the reward was calculated to be 3100.88$ with 23 cycles. 
							Increased window size (48hr) resulted in a slightly better return of 3184.32$, reaching the limit with the same cycle count. 
						</p>
						<img src="Assets/EMA_test_result.png" style="max-width: 100%; max-height: 100%;">
					</div>


				</div>

				<br/>
				<br/>
				<br/>

				<div class="row justify-content-center ">
						<br/>
						<h4> <b> DQN and DDQN </b></h4>
						<p>
							The agents trained using the 1st strategy returned a similar 
							total reward in the medium and long sequences. DDQN trained in the mid-range 
							produced a slightly more reward, toping 8064.00$ with 10 battery-cycle count. 
							This result indicates that deep reinforcement learning produced a 2.5x greater return then the EMA. 
						</p>
					
					<div class="row justify-content-center ">
						<div  class="col-lg-8 col-md-8 col-sm-8">
							<img src="Assets/S1_rewards.png" style="max-width: 100%; max-height: 100%; margin-bottom: 10px;">
						</div>
						<div  class="col-lg-4 col-md-4 col-sm-4 ">	
							<img src="Assets/S1_cycles.png" style="max-width: 100%; max-height: 100%; margin-bottom: 10px;">
						</div>
					</div>
				</div>

				<div>
					<p>
						While analysing the results using the test set, 
						models trained with the 2nd and 3rd strategies‚Äô rewards were also calculated using the 1st strategy. 
					</p>
					<p>
						Following the 2nd and 3rd strategies, although the DQN and DDQN performances on short episodes are 
						not distinctive, DQN agents trained in the longer episodes produced a significant performance drop. 
						With the increased episode lengths, DQN became unstable and suffered from maximization bias. The DDQN 
						architecture overcame this challenge producing significantly improved outcomes with a much stable strategy. 
						The DDQN agent trained using the 3rd strategy in long episodes returned the maximum reward of 9136.00$ with only 8 cycles. 
					</p>
				</div>
				<div class="row justify-content-center ">
					<div  class="col-lg-6 col-md-6 col-sm-12">
						<img src="Assets/S2_rewards.png" style="max-width: 100%; max-height: 50%; margin-bottom: 20px;">
						<img src="Assets/S2_cycles.png" style="max-width: 100%; max-height: 50%; margin-bottom: 20px;">
					</div>
					<div  class="col-lg-6 col-md-6 col-sm-12 ">	
						<div class="row justify-content-center ">

							<img src="Assets/S3_rewards.png" style="max-width: 100%; max-height: 50%; margin-bottom: 20px;">
							<img src="Assets/S3_cycles.png" style="max-width: 100%; max-height: 50%; margin-bottom: 20px;">
						</div>
					</div>
				</div>

				</div>

			</div>
		</div>
										<!-- END -->
		<div class="footer">
			<p>Kaan Gun Feb. 2021</p>
		</div>
	</div>

	<script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.4/dist/Chart.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/onnxjs/dist/onnx.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta2/dist/js/bootstrap.bundle.min.js" integrity="sha384-b5kHyXgcpbZJO/tY9Ul7kGkf1S0CWuKcCD38l8YkeH8z8QjE0GmW1gYU5S9FOnJ0" crossorigin="anonymous"></script>
	<script src="src/init.js"></script>

</body>
</html>
